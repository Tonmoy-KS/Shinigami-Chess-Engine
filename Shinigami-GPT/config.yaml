# config.yaml

# Architecture
block_size: 2048
vocab_size: 10000
n_layer: 12
n_head: 12
n_kv_heads: 12 
n_embed: 768   
dropout: 0.1
untie_weights: true
use_bias: false

# MoE Specific (set num_experts to 0 to disable MoE)
moe:
  num_experts: 8
  num_experts_per_tok: 2

# RoPE Scaling (for extending context during inference)
rope_scaling:
  type: "linear" # "linear" or "ntk"
  factor: 2.0   # e.g., 2.0 means it can handle 2x the block_size

# Flash Attention (set to false to fallback to default attention)
use_flash_attn: true

# Draft Model (for speculative decoding)
draft_model:
  tag: "draft-model"
  n_layer: 4
  n_head: 4
  n_kv_heads: 4
  n_embed: 256

# Training
training:
  global_batch_size: 128
  micro_batch_size: 8 # Per-GPU batch size
  max_iters: 5000
  # Optimizer defined in DeepSpeed config
  # LR Schedule defined in DeepSpeed config
  clip_grad: 1.0

# Evaluation
evaluation:
  interval: 250
  iters: 200

# Infrastructure & Checkpointing
infra:
  # Checkpoint paths are now managed by DeepSpeed
  checkpoint_dir: "out/checkpoints"
  use_amp: true       # AMP (bf16/fp16) is managed by DeepSpeed
  compile_model: false # torch.compile() is not fully compatible with DeepSpeed ZeRO

# W&B Logging
wandb:
  log: true
  project: 'llm-framework-deepspeed'
  run_name: 'ds-moe-run'

# Data
data:
  tokenizer_path: "bpe_tokenizer.json"
  dataset_path: "input.txt"

# Generation (for generate.py)
generation:
  temperature: 0.8
  top_k: 50
  speculative_k: 4