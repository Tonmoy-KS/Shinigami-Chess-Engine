# config.yaml

# Architecture
block_size: 2048
vocab_size: 10000
n_layer: 12
n_head: 12
n_kv_heads: 12 
n_embed: 768   
dropout: 0.1
untie_weights: true

# MoE Specific (set moe_num_experts to 0 to disable MoE and use a standard FFN)
moe:
  num_experts: 8
  num_experts_per_tok: 2

# Flash Attention (set to false to fallback to default attention)
use_flash_attn: true

# Training
training:
  batch_size: 16
  max_iters: 5000
  grad_accum_steps: 4
  # Optimizer
  learning_rate: 3.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  clip_grad: 1.0
  # LR Schedule
  decay_lr: true
  warmup_iters: 200
  min_lr: 3.0e-5

# Evaluation
evaluation:
  interval: 250
  iters: 200

# Infrastructure & Checkpointing
infra:
  device: 'cuda'
  ckpt_path: "out/checkpoint.pt" # Checkpoints will be saved here
  use_amp: true
  compile_model: false # Experimental: torch.compile()

# W&B Logging
wandb:
  log: true
  project: 'llm-framework-final'
  run_name: 'fsdp-moe-run'

# Data
data:
  tokenizer_path: "bpe_tokenizer.json"
  dataset_path: "input.txt"

# Generation (for generate.py)
generation:
  temperature: 0.8
  top_k: 50
