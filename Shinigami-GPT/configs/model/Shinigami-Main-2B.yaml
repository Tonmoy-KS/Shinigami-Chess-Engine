# Parameter Calculation Approximation:
# - Embeddings (Untied): 2 * vocab_size * n_embed = 2 * 10000 * 2560 = 51.2M
# - Attention (per layer): 4 * n_embed^2 = 4 * 2560^2 = 26.2M
# - FFN (per layer): ~3 * n_embed * hidden_dim (where hidden_dim is ~8/3 * n_embed) = ~8 * n_embed^2 = 52.4M
# - Total per layer: ~78.6M
# - All Layers: 26 * 78.6M = 2.04B
# - Grand Total: 51.2M + 2.04B ~= 2.1 Billion parameters

name: "Shinigami-Main-2B"

# The embedding dimension is the primary driver of parameter count.
# Increased from 768 to 2560.
n_embed: 2560

# The number of layers (depth) is also increased significantly.
# Increased from 12 to 26.
n_layer: 26

# Number of heads is increased to match the larger embedding dimension.
n_head: 32
n_kv_heads: 32 # Using standard Multi-Head Attention (MHA)

# --- Other parameters kept consistent with the base model ---
block_size: 2048
vocab_size: 10000
dropout: 0.1
untie_weights: true
use_bias: false

# Sliding Window Attention (SWA) for inference
sliding_window_size: 4096

# MoE is disabled for this standard dense model
moe:
  num_experts: 0
  num_experts_per_tok: 0

rope_scaling:
  type: "linear"
  factor: 2.0
