name: "Shinigami-GPT-768M"
block_size: 2048
vocab_size: 10000
n_layer: 12
n_head: 12
n_kv_heads: 12 
n_embed: 768   
dropout: 0.1
untie_weights: true
use_bias: false

# Sliding Window Attention (SWA) for inference
# Set to 0 or a value > block_size to disable and use full attention
sliding_window_size: 4096

moe:
  num_experts: 0 # Disabled for standard model
  num_experts_per_tok: 0

rope_scaling:
  type: "linear"
  factor: 2.0
